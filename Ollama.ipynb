{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/tom522118/colab/blob/main/Ollama.ipynb","timestamp":1716334944547}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import os\n","os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})"],"metadata":{"id":"MmsngdhSplq_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! curl -fsSL https://ollama.com/install.sh | sed 's#https://ollama.com/download#https://github.com/jmorganca/ollama/releases/download/v0.1.27#' | sh"],"metadata":{"id":"O5lnmJbbSsB1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install ngrok\n","!pip install pyngrok"],"metadata":{"id":"jqjSu782QDaU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import threading\n","import time\n","import os\n","import asyncio\n","from pyngrok import ngrok\n","import threading\n","import queue\n","import time\n","from threading import Thread\n","\n","# Get your ngrok token from your ngrok account:\n","# https://dashboard.ngrok.com/get-started/your-authtoken\n","token=\"2gn0un589GmjkXaOvhPXlDGGZyu_4juTRhGmzGPZiFqTEsrBF\"\n","ngrok.set_auth_token(token)\n","\n","# set up a stoppable thread (not mandatory, but cleaner if you want to stop this later\n","class StoppableThread(threading.Thread):\n","    def __init__(self, *args, **kwargs):\n","        super(StoppableThread, self).__init__(*args, **kwargs)\n","        self._stop_event = threading.Event()\n","\n","    def stop(self):\n","        self._stop_event.set()\n","\n","    def is_stopped(self):\n","        return self._stop_event.is_set()\n","\n","def start_ngrok(q, stop_event):\n","    try:\n","        # Start an HTTP tunnel on the specified port\n","        public_url = ngrok.connect(11434)\n","        # Put the public URL in the queue\n","        q.put(public_url)\n","        # Keep the thread alive until stop event is set\n","        while not stop_event.is_set():\n","            time.sleep(1)  # Adjust sleep time as needed\n","    except Exception as e:\n","        print(f\"Error in start_ngrok: {e}\")"],"metadata":{"id":"e-Jqo78YQa1j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a queue to share data between threads\n","url_queue = queue.Queue()\n","\n","# Start ngrok in a separate thread\n","ngrok_thread = StoppableThread(target=start_ngrok, args=(url_queue, StoppableThread.is_stopped))\n","ngrok_thread.start()"],"metadata":{"id":"A0VwP3obQtNk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Wait for the ngrok tunnel to be established\n","while True:\n","    try:\n","        public_url = url_queue.get()\n","        if public_url:\n","            break\n","        print(\"Waiting for ngrok URL...\")\n","        time.sleep(1)\n","    except Exception as e:\n","        print(f\"Error in retrieving ngrok URL: {e}\")\n","\n","print(\"Ngrok tunnel established at:\", public_url)"],"metadata":{"id":"593QBBTIQwgD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import asyncio\n","\n","# NB: You may need to set these depending and get cuda working depending which backend you are running.\n","# Set environment variable for NVIDIA library\n","# Set environment variables for CUDA\n","os.environ['PATH'] += ':/usr/local/cuda/bin'\n","# Set LD_LIBRARY_PATH to include both /usr/lib64-nvidia and CUDA lib directories\n","os.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia:/usr/local/cuda/lib64'\n","\n","async def run_process(cmd):\n","    print('>>> starting', *cmd)\n","    process = await asyncio.create_subprocess_exec(\n","        *cmd,\n","        stdout=asyncio.subprocess.PIPE,\n","        stderr=asyncio.subprocess.PIPE\n","    )\n","\n","    # define an async pipe function\n","    async def pipe(lines):\n","        async for line in lines:\n","            print(line.decode().strip())\n","\n","        await asyncio.gather(\n","            pipe(process.stdout),\n","            pipe(process.stderr),\n","        )\n","\n","    # call it\n","    await asyncio.gather(pipe(process.stdout), pipe(process.stderr))"],"metadata":{"id":"AJM2B2EUQ4Cc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import asyncio\n","import threading\n","\n","async def start_ollama_serve():\n","    await run_process(['ollama', 'serve'])\n","\n","def run_async_in_thread(loop, coro):\n","    asyncio.set_event_loop(loop)\n","    loop.run_until_complete(coro)\n","    loop.close()\n","\n","# Create a new event loop that will run in a new thread\n","new_loop = asyncio.new_event_loop()\n","\n","# Start ollama serve in a separate thread so the cell won't block execution\n","thread = threading.Thread(target=run_async_in_thread, args=(new_loop, start_ollama_serve()))\n","thread.start()"],"metadata":{"id":"M-sY5fGGQ8As"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! export OLLAMA_HOST=https://bb14-34-87-167-53.ngrok-free.app/"],"metadata":{"id":"FexmvZ9aRBjM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ollama pull llama2"],"metadata":{"id":"AT6z9qOzRJK7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!curl -X POST https://bb14-34-87-167-53.ngrok-free.app/api/generate -d '{ \"model\": \"llama2\", \"prompt\":\"Good Morning\" , \"stream\": false }'"],"metadata":{"id":"lGf2TTqeRN99"},"execution_count":null,"outputs":[]}]}